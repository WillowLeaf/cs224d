{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 224D Assignment #2\n",
    "# Part [2]: Recurrent Neural Networks\n",
    "\n",
    "This notebook will provide starter code, testing snippets, and additional guidance for implementing the Recurrent Neural Network Language Model (RNNLM) described in Part 2 of the handout.\n",
    "\n",
    "Please complete parts (a), (b), and (c) of Part 2 before beginning this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from numpy import *\n",
    "from matplotlib.pyplot import *\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['savefig.dpi'] = 100\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e): Implement a Recurrent Neural Network Language Model\n",
    "\n",
    "Follow the instructions on the handout to implement your model in `rnnlm.py`, then use the code below to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grad_check: dJ/dH_4 error norm = 2.748e-10 [ok]\n",
      "    H_4 dims: [10, 10] = 100 elem\n",
      "grad_check: dJ/dH_2 error norm = 1.954e-10 [ok]\n",
      "    H_2 dims: [10, 10] = 100 elem\n",
      "grad_check: dJ/dH_3 error norm = 1.49e-10 [ok]\n",
      "    H_3 dims: [10, 10] = 100 elem\n",
      "grad_check: dJ/dH_1 error norm = 1.565e-10 [ok]\n",
      "    H_1 dims: [10, 10] = 100 elem\n",
      "grad_check: dJ/db_4 error norm = 3.912e-10 [ok]\n",
      "    b_4 dims: [10] = 10 elem\n",
      "grad_check: dJ/dW_4 error norm = 2.734e-10 [ok]\n",
      "    W_4 dims: [10, 10] = 100 elem\n",
      "grad_check: dJ/dW_5 error norm = 1.792e-09 [ok]\n",
      "    W_5 dims: [20, 10] = 200 elem\n",
      "grad_check: dJ/db_1 error norm = 4.656e-11 [ok]\n",
      "    b_1 dims: [10] = 10 elem\n",
      "grad_check: dJ/db_2 error norm = 4.143e-11 [ok]\n",
      "    b_2 dims: [10] = 10 elem\n",
      "grad_check: dJ/db_3 error norm = 8.018e-11 [ok]\n",
      "    b_3 dims: [10] = 10 elem\n",
      "grad_check: dJ/dW_1 error norm = 1.237e-10 [ok]\n",
      "    W_1 dims: [10, 20] = 200 elem\n",
      "grad_check: dJ/db_5 error norm = 2.607e-09 [ok]\n",
      "    b_5 dims: [20] = 20 elem\n",
      "grad_check: dJ/dW_3 error norm = 1.792e-10 [ok]\n",
      "    W_3 dims: [10, 10] = 100 elem\n",
      "grad_check: dJ/dW_2 error norm = 1.662e-10 [ok]\n",
      "    W_2 dims: [10, 10] = 100 elem\n"
     ]
    }
   ],
   "source": [
    "from rnnlm2 import RNNLM\n",
    "# Gradient check on toy data, for speed\n",
    "random.seed(10)\n",
    "wv_dummy = random.randn(10,50)\n",
    "model = RNNLM((20, 10, 10, 20),\n",
    "              alpha=0.005, rseed=10)\n",
    "model.grad_check(array([1,2,3,4,5,6,7,8]), array([2,3,4,5,6,7,8,9]), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Vocabulary and Load PTB Data\n",
    "\n",
    "We've pre-prepared a list of the vocabulary in the Penn Treebank, along with their absolute counts and unigram frequencies. The document loader code below will \"canonicalize\" words and replace any unknowns with a `\"UUUNKKK\"` token, then convert the data to lists of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retained 2000 words from 38444 (84.00% of all tokens)\n"
     ]
    }
   ],
   "source": [
    "from data_utils import utils as du\n",
    "import pandas as pd\n",
    "\n",
    "# Load the vocabulary\n",
    "vocab = pd.read_table(\"data/lm/vocab.ptb.txt\", header=None, sep=\"\\s+\",\n",
    "                     index_col=0, names=['count', 'freq'], )\n",
    "\n",
    "# Choose how many top words to keep\n",
    "vocabsize = 2000\n",
    "num_to_word = dict(enumerate(vocab.index[:vocabsize]))\n",
    "word_to_num = du.invert_dict(num_to_word)\n",
    "##\n",
    "# Below needed for 'adj_loss': DO NOT CHANGE\n",
    "fraction_lost = float(sum([vocab['count'][word] for word in vocab.index\n",
    "                           if (not word in word_to_num) \n",
    "                               and (not word == \"UUUNKKK\")]))\n",
    "fraction_lost /= sum([vocab['count'][word] for word in vocab.index\n",
    "                      if (not word == \"UUUNKKK\")])\n",
    "print \"Retained %d words from %d (%.02f%% of all tokens)\" % (vocabsize, len(vocab),\n",
    "                                                             100*(1-fraction_lost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the datasets, using the vocabulary in `word_to_num`. Our starter code handles this for you, and also generates lists of lists X and Y, corresponding to input words and target words*. \n",
    "\n",
    "*(Of course, the target words are just the input words, shifted by one position, but it can be cleaner and less error-prone to keep them separate.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big investment banks refused to step up to the plate to support the beleaguered floor traders by buying big blocks of stock , traders say .\n",
      "[   4  147  169  250 1879    7 1224   64    7    1    3    7  456    1    3\n",
      " 1024  255   24  378  147    3    6   67    0  255  138    2    5]\n"
     ]
    }
   ],
   "source": [
    "# Load the training set\n",
    "docs = du.load_dataset('data/lm/ptb-train.txt')\n",
    "S_train = du.docs_to_indices(docs, word_to_num)\n",
    "X_train, Y_train = du.seqs_to_lmXY(S_train)\n",
    "\n",
    "# Load the dev set (for tuning hyperparameters)\n",
    "docs = du.load_dataset('data/lm/ptb-dev.txt')\n",
    "S_dev = du.docs_to_indices(docs, word_to_num)\n",
    "X_dev, Y_dev = du.seqs_to_lmXY(S_dev)\n",
    "\n",
    "# Load the test set (final evaluation only)\n",
    "docs = du.load_dataset('data/lm/ptb-test.txt')\n",
    "S_test = du.docs_to_indices(docs, word_to_num)\n",
    "X_test, Y_test = du.seqs_to_lmXY(S_test)\n",
    "\n",
    "# Display some sample data\n",
    "print \" \".join(d[0] for d in docs[7])\n",
    "print S_test[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f): Train and evaluate your model\n",
    "\n",
    "When you're able to pass the gradient check, let's run our model on some real language!\n",
    "\n",
    "You should randomly initialize the word vectors as Gaussian noise, i.e. $L_{ij} \\sim \\mathit{N}(0,0.1)$ and $U_{ij} \\sim \\mathit{N}(0,0.1)$; the function `random.randn` may be helpful here.\n",
    "\n",
    "As in Part 1, you should tune hyperparameters to get a good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grad_check: dJ/dH error norm = 1.007e-09 [ok]\n",
      "    H dims: [100, 100] = 10000 elem\n",
      "grad_check: dJ/dU error norm = 4.23e-09 [ok]\n",
      "    U dims: [2000, 100] = 200000 elem\n",
      "grad_check: dJ/dL[3] error norm = 1.873e-10 [ok]\n",
      "    L[3] dims: [100] = 100 elem\n",
      "grad_check: dJ/dL[2] error norm = 1.809e-10 [ok]\n",
      "    L[2] dims: [100] = 100 elem\n",
      "grad_check: dJ/dL[1] error norm = 2.361e-10 [ok]\n",
      "    L[1] dims: [100] = 100 elem\n"
     ]
    }
   ],
   "source": [
    "hdim = 100 # dimension of hidden layer = dimension of word vectors\n",
    "random.seed(10)\n",
    "L0 = 0.1 * random.randn(vocabsize, hdim) # replace with random init, \n",
    "                              # or do in RNNLM.__init__()\n",
    "# test parameters; you probably want to change these\n",
    "model = RNNLM(L0, U0 = L0, alpha=0.1, rseed=10, bptt=1)\n",
    "\n",
    "# Gradient check is going to take a *long* time here\n",
    "# since it's quadratic-time in the number of parameters.\n",
    "# run at your own risk... (but do check this!)\n",
    "model.grad_check(array([1,2,3]), array([2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 4.517\n",
      "  Seen 100 in 65.98 s\n",
      "  Seen 200 in 81.90 s\n",
      "  Seen 300 in 98.76 s\n",
      "  Seen 400 in 116.72 s\n",
      "  Seen 500 in 131.13 s\n",
      "  Seen 600 in 145.05 s\n",
      "  Seen 700 in 160.55 s\n",
      "  Seen 800 in 178.65 s\n",
      "  Seen 900 in 197.17 s\n",
      "  Seen 1000 in 215.86 s\n",
      "  Seen 1100 in 234.50 s\n",
      "  Seen 1200 in 251.70 s\n",
      "  Seen 1300 in 266.74 s\n",
      "  Seen 1400 in 283.57 s\n",
      "  Seen 1500 in 300.16 s\n",
      "  Seen 1600 in 318.18 s\n",
      "  Seen 1700 in 333.07 s\n",
      "  Seen 1800 in 345.99 s\n",
      "  Seen 1900 in 359.62 s\n",
      "  Seen 2000 in 372.47 s\n",
      "  Seen 2100 in 385.85 s\n",
      "  Seen 2200 in 398.36 s\n",
      "  Seen 2300 in 410.98 s\n",
      "  Seen 2400 in 423.80 s\n",
      "  Seen 2500 in 436.94 s\n",
      "  Seen 2600 in 450.42 s\n",
      "  Seen 2700 in 462.62 s\n",
      "  Seen 2800 in 475.50 s\n",
      "  Seen 2900 in 488.44 s\n",
      "  Seen 3000 in 500.90 s\n",
      "  Seen 3100 in 513.71 s\n",
      "  Seen 3200 in 526.18 s\n",
      "  Seen 3300 in 539.03 s\n",
      "  Seen 3400 in 552.01 s\n",
      "  Seen 3500 in 565.02 s\n",
      "  Seen 3600 in 578.81 s\n",
      "  Seen 3700 in 592.19 s\n",
      "  Seen 3800 in 605.14 s\n",
      "  Seen 3900 in 617.29 s\n",
      "  Seen 4000 in 637.32 s\n",
      "  Seen 4100 in 666.61 s\n",
      "  Seen 4200 in 694.95 s\n",
      "  Seen 4300 in 711.90 s\n",
      "  Seen 4400 in 727.36 s\n",
      "  Seen 4500 in 743.70 s\n",
      "  Seen 4600 in 758.77 s\n",
      "  Seen 4700 in 773.87 s\n",
      "  Seen 4800 in 789.97 s\n",
      "  Seen 4900 in 805.67 s\n",
      "  Seen 5000 in 821.20 s\n",
      "  [5000]: mean loss 3.70269\n",
      "  Seen 5100 in 887.67 s\n",
      "  Seen 5200 in 903.50 s\n",
      "  Seen 5300 in 919.08 s\n",
      "  Seen 5400 in 936.05 s\n",
      "  Seen 5500 in 951.16 s\n",
      "  Seen 5600 in 966.26 s\n",
      "  Seen 5700 in 981.57 s\n",
      "  Seen 5800 in 997.20 s\n",
      "  Seen 5900 in 1012.43 s\n",
      "  Seen 6000 in 1027.44 s\n",
      "  Seen 6100 in 1042.52 s\n",
      "  Seen 6200 in 1057.61 s\n",
      "  Seen 6300 in 1072.66 s\n",
      "  Seen 6400 in 1087.94 s\n",
      "  Seen 6500 in 1103.70 s\n",
      "  Seen 6600 in 1119.00 s\n",
      "  Seen 6700 in 1134.33 s\n",
      "  Seen 6800 in 1149.88 s\n",
      "  Seen 6900 in 1165.21 s\n",
      "  Seen 7000 in 1180.81 s\n",
      "  Seen 7100 in 1196.23 s\n",
      "  Seen 7200 in 1211.61 s\n",
      "  Seen 7300 in 1227.27 s\n",
      "  Seen 7400 in 1242.71 s\n",
      "  Seen 7500 in 1258.29 s\n",
      "  Seen 7600 in 1274.13 s\n",
      "  Seen 7700 in 1290.12 s\n",
      "  Seen 7800 in 1304.54 s\n",
      "  Seen 7900 in 1319.80 s\n",
      "  Seen 8000 in 1335.03 s\n",
      "  Seen 8100 in 1349.86 s\n",
      "  Seen 8200 in 1365.56 s\n",
      "  Seen 8300 in 1380.90 s\n",
      "  Seen 8400 in 1395.88 s\n",
      "  Seen 8500 in 1410.84 s\n",
      "  Seen 8600 in 1426.11 s\n",
      "  Seen 8700 in 1441.54 s\n",
      "  Seen 8800 in 1457.03 s\n",
      "  Seen 8900 in 1472.38 s\n",
      "  Seen 9000 in 1487.85 s\n",
      "  Seen 9100 in 1502.93 s\n",
      "  Seen 9200 in 1517.58 s\n",
      "  Seen 9300 in 1533.16 s\n",
      "  Seen 9400 in 1548.88 s\n",
      "  Seen 9500 in 1564.13 s\n",
      "  Seen 9600 in 1579.16 s\n",
      "  Seen 9700 in 1594.89 s\n",
      "  Seen 9800 in 1609.74 s\n",
      "  Seen 9900 in 1625.01 s\n",
      "  Seen 10000 in 1641.33 s\n",
      "  [10000]: mean loss 3.48707\n",
      "  Seen 10100 in 1708.50 s\n",
      "  Seen 10200 in 1724.75 s\n",
      "  Seen 10300 in 1740.58 s\n",
      "  Seen 10400 in 1756.40 s\n",
      "  Seen 10500 in 1772.06 s\n",
      "  Seen 10600 in 1787.53 s\n",
      "  Seen 10700 in 1809.48 s\n",
      "  Seen 10800 in 1827.86 s\n",
      "  Seen 10900 in 1840.74 s\n",
      "  Seen 11000 in 1853.65 s\n",
      "  Seen 11100 in 1870.74 s\n",
      "  Seen 11200 in 1886.85 s\n",
      "  Seen 11300 in 1900.35 s\n",
      "  Seen 11400 in 1915.11 s\n",
      "  Seen 11500 in 1930.80 s\n",
      "  Seen 11600 in 1944.76 s\n",
      "  Seen 11700 in 1959.72 s\n",
      "  Seen 11800 in 1974.85 s\n",
      "  Seen 11900 in 1989.60 s\n",
      "  Seen 12000 in 2004.37 s\n",
      "  Seen 12100 in 2018.52 s\n",
      "  Seen 12200 in 2033.60 s\n",
      "  Seen 12300 in 2048.20 s\n",
      "  Seen 12400 in 2062.69 s\n",
      "  Seen 12500 in 2076.77 s\n",
      "  Seen 12600 in 2091.72 s\n",
      "  Seen 12700 in 2106.14 s\n",
      "  Seen 12800 in 2119.96 s\n",
      "  Seen 12900 in 2134.08 s\n",
      "  Seen 13000 in 2148.46 s\n",
      "  Seen 13100 in 2164.00 s\n",
      "  Seen 13200 in 2178.49 s\n",
      "  Seen 13300 in 2195.11 s\n",
      "  Seen 13400 in 2211.43 s\n",
      "  Seen 13500 in 2228.40 s\n",
      "  Seen 13600 in 2243.57 s\n",
      "  Seen 13700 in 2259.50 s\n",
      "  Seen 13800 in 2277.70 s\n",
      "  Seen 13900 in 2295.89 s\n",
      "  Seen 14000 in 2311.81 s\n",
      "  Seen 14100 in 2329.14 s\n",
      "  Seen 14200 in 2344.21 s\n",
      "  Seen 14300 in 2360.16 s\n",
      "  Seen 14400 in 2377.13 s\n",
      "  Seen 14500 in 2393.78 s\n",
      "  Seen 14600 in 2412.15 s\n",
      "  Seen 14700 in 2427.67 s\n",
      "  Seen 14800 in 2443.10 s\n",
      "  Seen 14900 in 2463.49 s\n",
      "  Seen 15000 in 2481.92 s\n",
      "  [15000]: mean loss 3.36985\n",
      "  Seen 15100 in 2557.56 s\n",
      "  Seen 15200 in 2573.34 s\n",
      "  Seen 15300 in 2591.78 s\n",
      "  Seen 15400 in 2606.82 s\n",
      "  Seen 15500 in 2621.19 s\n",
      "  Seen 15600 in 2637.26 s\n",
      "  Seen 15700 in 2652.23 s\n",
      "  Seen 15800 in 2666.65 s\n",
      "  Seen 15900 in 2679.77 s\n",
      "SGD Interrupted: saw 15972 examples in 2689.38 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 4.5170011431387289),\n",
       " (5000, 3.7026867506582128),\n",
       " (10000, 3.487073333074262),\n",
       " (15000, 3.3698461435746432)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "\n",
    "##\n",
    "# Pare down to a smaller dataset, for speed\n",
    "# (optional - recommended to not do this for your final model)\n",
    "ntrain = 5000#len(Y_train)\n",
    "X = X_train[:ntrain]\n",
    "Y = Y_train[:ntrain]\n",
    "model.train_sgd(X, Y, idxiter=model.randomiter(ntrain * 5, ntrain, 5), alphaiter=None, \n",
    "                printevery=100, costevery=5000, devidx=None)\n",
    "\n",
    "\n",
    "\n",
    "#### END YOUR CODE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Evaluate cross-entropy loss on the dev set,\n",
    "## then convert to perplexity for your writeup\n",
    "dev_loss = model.compute_mean_loss(X_dev, Y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the model is skewed somewhat by the large number of `UUUNKKK` tokens; if these are 1/6 of the dataset, then that's a sizeable fraction that we're just waving our hands at. Naively, our model gets credit for these that's not really deserved; the formula below roughly removes this contribution from the average loss. Don't worry about how it's derived, but do report both scores - it helps us compare across models with different vocabulary sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unadjusted: 137.336\n",
      "Adjusted for missing vocab: 247.348\n"
     ]
    }
   ],
   "source": [
    "## DO NOT CHANGE THIS CELL ##\n",
    "# Report your numbers, after computing dev_loss above.\n",
    "def adjust_loss(loss, funk, q, mode='basic'):\n",
    "    if mode == 'basic':\n",
    "        # remove freebies only: score if had no UUUNKKK\n",
    "        return (loss + funk*log(funk))/(1 - funk)\n",
    "    else:\n",
    "        # remove freebies, replace with best prediction on remaining\n",
    "        return loss + funk*log(funk) - funk*log(q)\n",
    "# q = best unigram frequency from omitted vocab\n",
    "# this is the best expected loss out of that set\n",
    "q = vocab.freq[vocabsize] / sum(vocab.freq[vocabsize:])\n",
    "print \"Unadjusted: %.03f\" % exp(dev_loss)\n",
    "print \"Adjusted for missing vocab: %.03f\" % exp(adjust_loss(dev_loss, fraction_lost, q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "# Save to .npy files; should only be a few MB total\n",
    "assert(min(model.sparams.L.shape) <= 100) # don't be too big\n",
    "assert(max(model.sparams.L.shape) <= 5000) # don't be too big\n",
    "save(\"rnnlm.L.npy\", model.sparams.L)\n",
    "save(\"rnnlm.U.npy\", model.params.U)\n",
    "save(\"rnnlm.H.npy\", model.params.H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (g): Generating Data\n",
    "\n",
    "Once you've trained your model to satisfaction, let's use it to generate some sentences!\n",
    "\n",
    "Implement the `generate_sequence` function in `rnnlm.py`, and call it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.4488996666\n",
      "<s> shares fell dec. DG . </s>\n"
     ]
    }
   ],
   "source": [
    "def seq_to_words(seq):\n",
    "    return [num_to_word[s] for s in seq]\n",
    "    \n",
    "seq, J = model.generate_sequence(word_to_num[\"<s>\"], \n",
    "                                 word_to_num[\"</s>\"], \n",
    "                                 maxlen=100)\n",
    "print J\n",
    "# print seq\n",
    "print \" \".join(seq_to_words(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BONUS:** Use the unigram distribution given in the `vocab` table to fill in any `UUUNKKK` tokens in your generated sequences with words that we omitted from the vocabulary. You'll want to use `list(vocab.index)` to get a list of words, and `vocab.freq` to get a list of corresponding frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace UUUNKKK with a random unigram,\n",
    "# drawn from vocab that we skipped\n",
    "from nn.math import MultinomialSampler, multinomial_sample\n",
    "def fill_unknowns(words):\n",
    "    #### YOUR CODE HERE ####\n",
    "    ret = words # do nothing; replace this\n",
    "    \n",
    "\n",
    "    #### END YOUR CODE ####\n",
    "    return ret\n",
    "    \n",
    "print \" \".join(fill_unknowns(seq_to_words(seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
