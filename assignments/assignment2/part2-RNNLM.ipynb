{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 224D Assignment #2\n",
    "# Part [2]: Recurrent Neural Networks\n",
    "\n",
    "This notebook will provide starter code, testing snippets, and additional guidance for implementing the Recurrent Neural Network Language Model (RNNLM) described in Part 2 of the handout.\n",
    "\n",
    "Please complete parts (a), (b), and (c) of Part 2 before beginning this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from numpy import *\n",
    "from matplotlib.pyplot import *\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['savefig.dpi'] = 100\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e): Implement a Recurrent Neural Network Language Model\n",
    "\n",
    "Follow the instructions on the handout to implement your model in `rnnlm.py`, then use the code below to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grad_check: dJ/dH error norm = 3.135e-09 [ok]\n",
      "    H dims: [50, 50] = 2500 elem\n",
      "grad_check: dJ/dU error norm = 5.377e-10 [ok]\n",
      "    U dims: [10, 50] = 500 elem\n",
      "grad_check: dJ/dL[3] error norm = 1.186e-09 [ok]\n",
      "    L[3] dims: [50] = 50 elem\n",
      "grad_check: dJ/dL[2] error norm = 9.955e-10 [ok]\n",
      "    L[2] dims: [50] = 50 elem\n",
      "grad_check: dJ/dL[1] error norm = 1.16e-09 [ok]\n",
      "    L[1] dims: [50] = 50 elem\n"
     ]
    }
   ],
   "source": [
    "from rnnlm import RNNLM\n",
    "# Gradient check on toy data, for speed\n",
    "random.seed(10)\n",
    "wv_dummy = random.randn(10,50)\n",
    "model = RNNLM(L0 = wv_dummy, U0 = wv_dummy,\n",
    "              alpha=0.005, rseed=10, bptt=4)\n",
    "model.grad_check(array([1,2,3]), array([2,3,4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Vocabulary and Load PTB Data\n",
    "\n",
    "We've pre-prepared a list of the vocabulary in the Penn Treebank, along with their absolute counts and unigram frequencies. The document loader code below will \"canonicalize\" words and replace any unknowns with a `\"UUUNKKK\"` token, then convert the data to lists of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retained 2000 words from 38444 (84.00% of all tokens)\n"
     ]
    }
   ],
   "source": [
    "from data_utils import utils as du\n",
    "import pandas as pd\n",
    "\n",
    "# Load the vocabulary\n",
    "vocab = pd.read_table(\"data/lm/vocab.ptb.txt\", header=None, sep=\"\\s+\",\n",
    "                     index_col=0, names=['count', 'freq'], )\n",
    "\n",
    "# Choose how many top words to keep\n",
    "vocabsize = 2000\n",
    "num_to_word = dict(enumerate(vocab.index[:vocabsize]))\n",
    "word_to_num = du.invert_dict(num_to_word)\n",
    "##\n",
    "# Below needed for 'adj_loss': DO NOT CHANGE\n",
    "fraction_lost = float(sum([vocab['count'][word] for word in vocab.index\n",
    "                           if (not word in word_to_num) \n",
    "                               and (not word == \"UUUNKKK\")]))\n",
    "fraction_lost /= sum([vocab['count'][word] for word in vocab.index\n",
    "                      if (not word == \"UUUNKKK\")])\n",
    "print \"Retained %d words from %d (%.02f%% of all tokens)\" % (vocabsize, len(vocab),\n",
    "                                                             100*(1-fraction_lost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the datasets, using the vocabulary in `word_to_num`. Our starter code handles this for you, and also generates lists of lists X and Y, corresponding to input words and target words*. \n",
    "\n",
    "*(Of course, the target words are just the input words, shifted by one position, but it can be cleaner and less error-prone to keep them separate.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19,) object\n",
      "Big investment banks refused to step up to the plate to support the beleaguered floor traders by buying big blocks of stock , traders say .\n",
      "[   4  147  169  250 1879    7 1224   64    7    1    3    7  456    1    3\n",
      " 1024  255   24  378  147    3    6   67    0  255  138    2    5]\n"
     ]
    }
   ],
   "source": [
    "# Load the training set\n",
    "docs = du.load_dataset('data/lm/ptb-train.txt')\n",
    "S_train = du.docs_to_indices(docs, word_to_num)\n",
    "X_train, Y_train = du.seqs_to_lmXY(S_train)\n",
    "\n",
    "# Load the dev set (for tuning hyperparameters)\n",
    "docs = du.load_dataset('data/lm/ptb-dev.txt')\n",
    "S_dev = du.docs_to_indices(docs, word_to_num)\n",
    "X_dev, Y_dev = du.seqs_to_lmXY(S_dev)\n",
    "\n",
    "# Load the test set (final evaluation only)\n",
    "docs = du.load_dataset('data/lm/ptb-test.txt')\n",
    "S_test = du.docs_to_indices(docs, word_to_num)\n",
    "X_test, Y_test = du.seqs_to_lmXY(S_test)\n",
    "print X[0].shape, X.dtype\n",
    "# Display some sample data\n",
    "print \" \".join(d[0] for d in docs[7])\n",
    "print S_test[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f): Train and evaluate your model\n",
    "\n",
    "When you're able to pass the gradient check, let's run our model on some real language!\n",
    "\n",
    "You should randomly initialize the word vectors as Gaussian noise, i.e. $L_{ij} \\sim \\mathit{N}(0,0.1)$ and $U_{ij} \\sim \\mathit{N}(0,0.1)$; the function `random.randn` may be helpful here.\n",
    "\n",
    "As in Part 1, you should tune hyperparameters to get a good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "grad_check: dJ/dH error norm = 1.007e-09 [ok]\n",
      "    H dims: [100, 100] = 10000 elem\n",
      "grad_check: dJ/dU error norm = 4.23e-09 [ok]\n",
      "    U dims: [2000, 100] = 200000 elem\n",
      "grad_check: dJ/dL[3] error norm = 1.873e-10 [ok]\n",
      "    L[3] dims: [100] = 100 elem\n",
      "grad_check: dJ/dL[2] error norm = 1.809e-10 [ok]\n",
      "    L[2] dims: [100] = 100 elem\n",
      "grad_check: dJ/dL[1] error norm = 2.361e-10 [ok]\n",
      "    L[1] dims: [100] = 100 elem\n"
     ]
    }
   ],
   "source": [
    "hdim = 100 # dimension of hidden layer = dimension of word vectors\n",
    "random.seed(10)\n",
    "L0 = 0.1 * random.randn(vocabsize, hdim) # replace with random init, \n",
    "                              # or do in RNNLM.__init__()\n",
    "# test parameters; you probably want to change these\n",
    "model = RNNLM(L0, U0 = L0, alpha=0.1, rseed=10, bptt=1)\n",
    "\n",
    "# Gradient check is going to take a *long* time here\n",
    "# since it's quadratic-time in the number of parameters.\n",
    "# run at your own risk... (but do check this!)\n",
    "model.grad_check(array([1,2,3]), array([2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin SGD...\n",
      "  Seen 0 in 0.00 s\n",
      "  [0]: mean loss 4.13891\n",
      "  Seen 100 in 55.43 s\n",
      "  Seen 200 in 58.48 s\n",
      "  Seen 300 in 61.49 s\n",
      "  Seen 400 in 64.86 s\n",
      "  Seen 500 in 67.94 s\n",
      "  Seen 600 in 70.68 s\n",
      "  Seen 700 in 73.61 s\n",
      "  Seen 800 in 76.25 s\n",
      "  Seen 900 in 78.91 s\n",
      "  Seen 1000 in 81.40 s\n",
      "  Seen 1100 in 84.13 s\n",
      "  Seen 1200 in 86.91 s\n",
      "  Seen 1300 in 89.34 s\n",
      "  Seen 1400 in 91.78 s\n",
      "  Seen 1500 in 94.27 s\n",
      "  Seen 1600 in 97.07 s\n",
      "  Seen 1700 in 99.44 s\n",
      "  Seen 1800 in 102.07 s\n",
      "  Seen 1900 in 104.49 s\n",
      "  Seen 2000 in 107.36 s\n",
      "  Seen 2100 in 110.06 s\n",
      "  Seen 2200 in 112.59 s\n",
      "  Seen 2300 in 115.22 s\n",
      "  Seen 2400 in 117.93 s\n",
      "  Seen 2500 in 120.61 s\n",
      "  Seen 2600 in 123.45 s\n",
      "  Seen 2700 in 126.03 s\n",
      "  Seen 2800 in 128.56 s\n",
      "  Seen 2900 in 131.24 s\n",
      "  Seen 3000 in 133.87 s\n",
      "  Seen 3100 in 136.72 s\n",
      "  Seen 3200 in 139.25 s\n",
      "  Seen 3300 in 141.68 s\n",
      "  Seen 3400 in 144.25 s\n",
      "  Seen 3500 in 148.40 s\n",
      "  Seen 3600 in 153.36 s\n",
      "  Seen 3700 in 155.97 s\n",
      "  Seen 3800 in 159.41 s\n",
      "  Seen 3900 in 162.22 s\n",
      "  Seen 4000 in 164.69 s\n",
      "  Seen 4100 in 167.54 s\n",
      "  Seen 4200 in 170.20 s\n",
      "  Seen 4300 in 172.69 s\n",
      "  Seen 4400 in 175.55 s\n",
      "  Seen 4500 in 178.00 s\n",
      "  Seen 4600 in 181.22 s\n",
      "  Seen 4700 in 185.84 s\n",
      "  Seen 4800 in 189.56 s\n",
      "  Seen 4900 in 192.99 s\n",
      "  Seen 5000 in 196.06 s\n",
      "  [5000]: mean loss 3.59959\n",
      "  Seen 5100 in 246.18 s\n",
      "  Seen 5200 in 248.60 s\n",
      "  Seen 5300 in 251.26 s\n",
      "  Seen 5400 in 254.13 s\n",
      "  Seen 5500 in 257.25 s\n",
      "  Seen 5600 in 259.84 s\n",
      "  Seen 5700 in 262.70 s\n",
      "  Seen 5800 in 265.91 s\n",
      "  Seen 5900 in 269.03 s\n",
      "  Seen 6000 in 272.13 s\n",
      "  Seen 6100 in 274.94 s\n",
      "  Seen 6200 in 277.74 s\n",
      "  Seen 6300 in 280.95 s\n",
      "  Seen 6400 in 283.98 s\n",
      "  Seen 6500 in 286.99 s\n",
      "  Seen 6600 in 290.12 s\n",
      "  Seen 6700 in 293.14 s\n",
      "  Seen 6800 in 296.17 s\n",
      "  Seen 6900 in 299.04 s\n",
      "  Seen 7000 in 301.77 s\n",
      "  Seen 7100 in 304.88 s\n",
      "  Seen 7200 in 307.39 s\n",
      "  Seen 7300 in 310.21 s\n",
      "  Seen 7400 in 313.29 s\n",
      "  Seen 7500 in 315.98 s\n",
      "  Seen 7600 in 319.16 s\n",
      "  Seen 7700 in 322.51 s\n",
      "  Seen 7800 in 325.71 s\n",
      "  Seen 7900 in 328.89 s\n",
      "  Seen 8000 in 332.07 s\n",
      "  Seen 8100 in 335.52 s\n",
      "  Seen 8200 in 337.86 s\n",
      "  Seen 8300 in 340.73 s\n",
      "  Seen 8400 in 343.71 s\n",
      "  Seen 8500 in 346.33 s\n",
      "  Seen 8600 in 349.16 s\n",
      "  Seen 8700 in 351.93 s\n",
      "  Seen 8800 in 354.77 s\n",
      "  Seen 8900 in 357.87 s\n",
      "  Seen 9000 in 360.70 s\n",
      "  Seen 9100 in 364.17 s\n",
      "  Seen 9200 in 367.08 s\n",
      "  Seen 9300 in 369.99 s\n",
      "  Seen 9400 in 373.17 s\n",
      "  Seen 9500 in 376.22 s\n",
      "  Seen 9600 in 379.17 s\n",
      "  Seen 9700 in 382.14 s\n",
      "  Seen 9800 in 384.94 s\n",
      "  Seen 9900 in 388.29 s\n",
      "  Seen 10000 in 391.51 s\n",
      "  [10000]: mean loss 3.6022\n",
      "  Seen 10100 in 440.28 s\n",
      "  Seen 10200 in 443.39 s\n",
      "  Seen 10300 in 446.36 s\n",
      "  Seen 10400 in 449.42 s\n",
      "  Seen 10500 in 452.48 s\n",
      "  Seen 10600 in 455.64 s\n",
      "  Seen 10700 in 458.50 s\n",
      "  Seen 10800 in 461.37 s\n",
      "  Seen 10900 in 464.26 s\n",
      "  Seen 11000 in 467.45 s\n",
      "  Seen 11100 in 470.45 s\n",
      "  Seen 11200 in 473.28 s\n",
      "  Seen 11300 in 476.59 s\n",
      "  Seen 11400 in 479.36 s\n",
      "  Seen 11500 in 482.25 s\n",
      "  Seen 11600 in 485.47 s\n",
      "  Seen 11700 in 488.32 s\n",
      "  Seen 11800 in 491.01 s\n",
      "  Seen 11900 in 493.83 s\n",
      "  Seen 12000 in 496.83 s\n",
      "  Seen 12100 in 499.77 s\n",
      "  Seen 12200 in 502.58 s\n",
      "  Seen 12300 in 505.34 s\n",
      "  Seen 12400 in 508.60 s\n",
      "  Seen 12500 in 513.53 s\n",
      "  Seen 12600 in 517.10 s\n",
      "  Seen 12700 in 520.58 s\n",
      "SGD Interrupted: saw 12703 examples in 520.83 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 4.1389059915439566),\n",
       " (5000, 3.599587441871845),\n",
       " (10000, 3.6021968779109028)]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "\n",
    "##\n",
    "# Pare down to a smaller dataset, for speed\n",
    "# (optional - recommended to not do this for your final model)\n",
    "ntrain = 5000#len(Y_train)\n",
    "X = X_train[:ntrain]\n",
    "Y = Y_train[:ntrain]\n",
    "model.train_sgd(X, Y, idxiter=model.randomiter(ntrain * 5, ntrain, 1), alphaiter=None, \n",
    "                printevery=100, costevery=5000, devidx=None)\n",
    "\n",
    "\n",
    "\n",
    "#### END YOUR CODE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Evaluate cross-entropy loss on the dev set,\n",
    "## then convert to perplexity for your writeup\n",
    "dev_loss = model.compute_mean_loss(X_dev, Y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the model is skewed somewhat by the large number of `UUUNKKK` tokens; if these are 1/6 of the dataset, then that's a sizeable fraction that we're just waving our hands at. Naively, our model gets credit for these that's not really deserved; the formula below roughly removes this contribution from the average loss. Don't worry about how it's derived, but do report both scores - it helps us compare across models with different vocabulary sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unadjusted: 137.336\n",
      "Adjusted for missing vocab: 247.348\n"
     ]
    }
   ],
   "source": [
    "## DO NOT CHANGE THIS CELL ##\n",
    "# Report your numbers, after computing dev_loss above.\n",
    "def adjust_loss(loss, funk, q, mode='basic'):\n",
    "    if mode == 'basic':\n",
    "        # remove freebies only: score if had no UUUNKKK\n",
    "        return (loss + funk*log(funk))/(1 - funk)\n",
    "    else:\n",
    "        # remove freebies, replace with best prediction on remaining\n",
    "        return loss + funk*log(funk) - funk*log(q)\n",
    "# q = best unigram frequency from omitted vocab\n",
    "# this is the best expected loss out of that set\n",
    "q = vocab.freq[vocabsize] / sum(vocab.freq[vocabsize:])\n",
    "print \"Unadjusted: %.03f\" % exp(dev_loss)\n",
    "print \"Adjusted for missing vocab: %.03f\" % exp(adjust_loss(dev_loss, fraction_lost, q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "# Save to .npy files; should only be a few MB total\n",
    "assert(min(model.sparams.L.shape) <= 100) # don't be too big\n",
    "assert(max(model.sparams.L.shape) <= 5000) # don't be too big\n",
    "save(\"rnnlm.L.npy\", model.sparams.L)\n",
    "save(\"rnnlm.U.npy\", model.params.U)\n",
    "save(\"rnnlm.H.npy\", model.params.H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (g): Generating Data\n",
    "\n",
    "Once you've trained your model to satisfaction, let's use it to generate some sentences!\n",
    "\n",
    "Implement the `generate_sequence` function in `rnnlm.py`, and call it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.4488996666\n",
      "<s> shares fell dec. DG . </s>\n"
     ]
    }
   ],
   "source": [
    "def seq_to_words(seq):\n",
    "    return [num_to_word[s] for s in seq]\n",
    "    \n",
    "seq, J = model.generate_sequence(word_to_num[\"<s>\"], \n",
    "                                 word_to_num[\"</s>\"], \n",
    "                                 maxlen=100)\n",
    "print J\n",
    "# print seq\n",
    "print \" \".join(seq_to_words(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BONUS:** Use the unigram distribution given in the `vocab` table to fill in any `UUUNKKK` tokens in your generated sequences with words that we omitted from the vocabulary. You'll want to use `list(vocab.index)` to get a list of words, and `vocab.freq` to get a list of corresponding frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace UUUNKKK with a random unigram,\n",
    "# drawn from vocab that we skipped\n",
    "from nn.math import MultinomialSampler, multinomial_sample\n",
    "def fill_unknowns(words):\n",
    "    #### YOUR CODE HERE ####\n",
    "    ret = words # do nothing; replace this\n",
    "    \n",
    "\n",
    "    #### END YOUR CODE ####\n",
    "    return ret\n",
    "    \n",
    "print \" \".join(fill_unknowns(seq_to_words(seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
